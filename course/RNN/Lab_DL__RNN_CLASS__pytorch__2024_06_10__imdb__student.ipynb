{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBFqSEkKqpCN"
      },
      "source": [
        "# Lab Deep Learning/ Recurrent Neural Networks/ in pytorch\n",
        "\n",
        "## Using Many-to-One for movie rating predicton\n",
        "\n",
        "**Author: created by geoffroy.peeters@telecom-paris.fr** with the help of Stéphane Lathuilière\n",
        "\n",
        "For any remark or suggestion, please feel free to contact me.\n",
        "\n",
        "## Objective:\n",
        "You will implement two different networks to perform the automatic rating (0 or 1) of movies given the text of their reviews.\n",
        "You will use the ```imdb``` (internet movie database) dataset.\n",
        "\n",
        "The reviews are already available in the form of indexes that point to a word dictionary: each word is already encoded as an index in the dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmkCSNaXLqjh"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "AOqjzDwioJj9"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from argparse import Namespace\n",
        "\n",
        "colab = True\n",
        "student = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Yp4OQVvUtr"
      },
      "source": [
        "## Parameters of the model\n",
        "\n",
        "- We only consider the most used words in the word dictionary, we consider the top `param.n_word`\n",
        "- We truncate/zero-pad each review to a length `param.T_x`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4C_Pv7rYvRkM"
      },
      "outputs": [],
      "source": [
        "param = Namespace()\n",
        "\n",
        "param.n_word = 5000 # --- input dimension\n",
        "param.T_x = 100 # --- review length\n",
        "param.index_word_from = 3 # --- indicate where the index start from (first index are used to indicate `PAD` `START` `UNK` tokens)\n",
        "\n",
        "param.n_embedding = 32 # --- dimension of the embedding\n",
        "param.n_lstm = 100 # --- dimension of the LSTM (for a<t> and c<t>)\n",
        "param.n_out = 1 # --- binary classification problem\n",
        "\n",
        "param.batch_size = 64\n",
        "param.lr = 0.001\n",
        "param.n_epoch = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayqpne5ULzDh",
        "outputId": "46ea1142-61d1-4f16-b354-a63beec797bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Namespace(n_word=5000, T_x=100, index_word_from=3, n_embedding=32, n_lstm=100, n_out=1, batch_size=64, lr=0.001, n_epoch=8)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsNcRimyLzgP"
      },
      "source": [
        "## Import IMDB data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gfe1ex8oN8Q",
        "outputId": "38f97779-d337-48a8-cc7d-f0bc25d16a5f"
      },
      "outputs": [],
      "source": [
        "# --- Import the IMDB data and only consider the ``param.n_word``` most used words\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=param.n_word, index_from=param.index_word_from )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSc5LmksOLyr"
      },
      "source": [
        "## Data content\n",
        "\n",
        "- ```X_train``` and ```X_test``` are each a numpy array, shape=(25000,), of lists.\n",
        "  - Each list represent a review; it is a sequence (represented as a list) of indexes (position of each word in the dictionary)\n",
        "\n",
        "- ```y_train``` and ```y_test``` are each a numpy array, shape=(25000,) of intergers.\n",
        "  - Each integer represent the values 0 (bad movie) or 1 (good movie)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "WouODCPrtiuu",
        "outputId": "e8114078-3417-45a5-eec7-12a07cace0a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type(X_train): <class 'list'>\n",
            "number of training sequences: X_train.shape: 25000\n",
            "type(X_train[0]): <class 'list'>\n",
            "length of the first training sequence: len(X_train[0]): 218\n",
            "length of the second training sequence: len(X_train[1]): 189\n",
            "list of data of the first training sequence: X_train[0]: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "maximum length of a training sequence: 2494\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"type(X_train):\", type(X_train))\n",
        "print(\"number of training sequences: X_train.shape:\", len(X_train))\n",
        "print(\"type(X_train[0]):\", type(X_train[0]))\n",
        "print(\"length of the first training sequence: len(X_train[0]):\",len(X_train[0]))\n",
        "print(\"length of the second training sequence: len(X_train[1]):\",len(X_train[1]))\n",
        "print(\"list of data of the first training sequence: X_train[0]:\", X_train[0] )\n",
        "len_list = [len(train) for train in X_train]\n",
        "print(\"maximum length of a training sequence:\", max(len_list))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(len_list, 100);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I-cEKUh_HM4"
      },
      "source": [
        "## Details of how the reviews are encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcOwiMUT_HM5",
        "outputId": "ae71dbce-d994-476c-fdc5-fb2b35c68a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<START> although i had seen <UNK> in a theater way back in <UNK> i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of <UNK> br br it turns out this is one of those films produced during the <UNK> that would go directly to video today the film stars <UNK> <UNK> kurt thomas as jonathan <UNK> <UNK> out of the blue to <UNK> the nation of <UNK> to enter and hopefully win the game a <UNK> <UNK> <UNK> by the khan who <UNK> his people by yelling what sounds like <UNK> power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess <UNK> who never speaks or leaves the house once trained tries to blend in with the <UNK> by wearing a bright red <UNK> with <UNK> of blue and white needless to say <UNK> finds himself running and fighting for his life along the stone streets of <UNK> on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert <UNK> who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many <UNK> throughout the town of <UNK> has a few good moments but is ultimately ruined by bad editing the ending <UNK> still there's the <UNK> of a good action adventure here a hong kong version with more <UNK> action and faster pace might even be pretty good\n"
          ]
        }
      ],
      "source": [
        "word_to_id = imdb.get_word_index()\n",
        "word_to_id = {key:(value+param.index_word_from) for key,value in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "print(' '.join(id_to_word[id] for id in X_train[1000] ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfl42LGCugWB",
        "outputId": "fcb4373a-05cd-411e-ad61-ef3648ea7040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type(y_train): <class 'numpy.ndarray'>\n",
            "y_train.shape: (25000,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"type(y_train):\", type(y_train))\n",
        "print(\"y_train.shape:\", y_train.shape)\n",
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVw65PNNuobX",
        "outputId": "69e51bfe-105c-48c8-c83f-9d3f57321522"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test.shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test.shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_test\u001b[38;5;241m.\u001b[39mshape)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V18OA7oQNH3c"
      },
      "source": [
        "## Data processing\n",
        "\n",
        "Sequences (represented as a list of values) in `X_train` represent the reviews.\n",
        "They can have different length $T_x$.\n",
        "To train the network we should modify them so that they all have the same length `param.T_x`.\n",
        "\n",
        "We do this by:\n",
        "- **truncating** the ones that are too long,\n",
        "- **padding-with-zeros** the ones that are too short.\n",
        "\n",
        "This can be done at the start of the sequence (`pre`) or at the end (`post`).\n",
        "\n",
        "In our use-case (rating of reviews), the decision ($\\hat{y}$) is taken after reading the whole sentence/review ${x^{<t>}}$. Therefore we will truncate and pad-with-zeroes in `pre` mode (truncate the beginning of the sequence if too long, or add zeroes add the beggining of the sequence if too short)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8UIF_RBUkfQI"
      },
      "outputs": [],
      "source": [
        "def do_pad_sequences(sequences, required_len, truncating='pre', padding='pre'):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "        sequences:  numpy arrays of lists, shape=(25000,)\n",
        "        required_len:     required length of each sequence after truncating and padding\n",
        "        padding     'pre' or 'post' mode\n",
        "        truncating  'pre' or 'post' mode\n",
        "    Returns\n",
        "    -------\n",
        "        padded_sequences    numpy arrays of lists (each list has now length maxlen)\n",
        "    \"\"\"\n",
        "    padded_sequences = []\n",
        "    if student:\n",
        "        # ---\n",
        "        for i in range(len(sequences)):\n",
        "          if(len(sequences[i]) == required_len):\n",
        "            padded_sequences.append(sequences[i])\n",
        "          elif (len(sequences[i]) > required_len):\n",
        "            # truncate\n",
        "            padded_sequences.append(sequences[i][-required_len:])\n",
        "          elif ( len(sequences[i]) < required_len):\n",
        "            len_ = len(sequences[i])\n",
        "            padded_sequences.append(np.concatenate( (np.zeros(required_len - len_), sequences[i][:] ) ))\n",
        "        padded_sequences = np.array(padded_sequences)\n",
        "        # ---\n",
        "    return padded_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCZ4yx2iWSvE",
        "outputId": "016c28c9-d40e-432f-fcdb-9d213625090a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 2,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 2,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 2,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 2,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 2,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhmiHsOGoRwT",
        "outputId": "444e0a6e-6555-4fd7-fd30-c09407098a54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(X_train[0]): 100\n",
            "len(X_train[1]): 100\n"
          ]
        }
      ],
      "source": [
        "# --- truncate and pad input sequences\n",
        "X_train = do_pad_sequences(X_train, required_len=param.T_x, padding='pre', truncating='pre')\n",
        "X_test = do_pad_sequences(X_test, required_len=param.T_x, padding='pre', truncating='pre')\n",
        "\n",
        "print(\"len(X_train[0]):\", len(X_train[0]))\n",
        "print(\"len(X_train[1]):\", len(X_train[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62ogKzKXTMBD",
        "outputId": "8db22634-62ef-4077-8744-c05fa4a4f013"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.415e+03, 3.300e+01, 6.000e+00, 2.200e+01, 1.200e+01, 2.150e+02,\n",
              "       2.800e+01, 7.700e+01, 5.200e+01, 5.000e+00, 1.400e+01, 4.070e+02,\n",
              "       1.600e+01, 8.200e+01, 2.000e+00, 8.000e+00, 4.000e+00, 1.070e+02,\n",
              "       1.170e+02, 2.000e+00, 1.500e+01, 2.560e+02, 4.000e+00, 2.000e+00,\n",
              "       7.000e+00, 3.766e+03, 5.000e+00, 7.230e+02, 3.600e+01, 7.100e+01,\n",
              "       4.300e+01, 5.300e+02, 4.760e+02, 2.600e+01, 4.000e+02, 3.170e+02,\n",
              "       4.600e+01, 7.000e+00, 4.000e+00, 2.000e+00, 1.029e+03, 1.300e+01,\n",
              "       1.040e+02, 8.800e+01, 4.000e+00, 3.810e+02, 1.500e+01, 2.970e+02,\n",
              "       9.800e+01, 3.200e+01, 2.071e+03, 5.600e+01, 2.600e+01, 1.410e+02,\n",
              "       6.000e+00, 1.940e+02, 2.000e+00, 1.800e+01, 4.000e+00, 2.260e+02,\n",
              "       2.200e+01, 2.100e+01, 1.340e+02, 4.760e+02, 2.600e+01, 4.800e+02,\n",
              "       5.000e+00, 1.440e+02, 3.000e+01, 2.000e+00, 1.800e+01, 5.100e+01,\n",
              "       3.600e+01, 2.800e+01, 2.240e+02, 9.200e+01, 2.500e+01, 1.040e+02,\n",
              "       4.000e+00, 2.260e+02, 6.500e+01, 1.600e+01, 3.800e+01, 1.334e+03,\n",
              "       8.800e+01, 1.200e+01, 1.600e+01, 2.830e+02, 5.000e+00, 1.600e+01,\n",
              "       4.472e+03, 1.130e+02, 1.030e+02, 3.200e+01, 1.500e+01, 1.600e+01,\n",
              "       2.000e+00, 1.900e+01, 1.780e+02, 3.200e+01])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TERlQPwZSKNH",
        "outputId": "0b6be9c3-1af9-4f62-d1c4-e93e55dda2a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train[0]: 100\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train[0]:\", len(X_train[20]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4oU71P6S1rI",
        "outputId": "b7a27eba-5f66-4283-adab-f349741ab6d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8IktAODF8g9"
      },
      "source": [
        "# Define training and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "g45DC3JEF72A"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, data_loader, criterion, optimizer):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    total_loss, total_acc = 0, 0\n",
        "    for X, y in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        hat_y = model(X)\n",
        "        loss = criterion(hat_y.squeeze(), y)\n",
        "        loss.backward() # --- SPECIFIC TO TRAINING\n",
        "        optimizer.step() # --- SPECIFIC TO TRAINING\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = (hat_y.squeeze() > 0.5).float()\n",
        "        total_acc += (predicted == y).sum().item()/len(y)\n",
        "\n",
        "    return total_loss/len(data_loader), total_acc/len(data_loader)\n",
        "\n",
        "\n",
        "\n",
        "def test_one_epoch(model, data_loader, criterion):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    total_loss, total_acc =  0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_loader:\n",
        "            hat_y = model(X)\n",
        "            loss = criterion(hat_y.squeeze(), y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predicted = (hat_y.squeeze() > 0.5).float()\n",
        "            total_acc += (predicted == y).sum().item()/len(y)\n",
        "\n",
        "    return total_loss/len(data_loader), total_acc/len(data_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, n_epoch):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(param.n_epoch):\n",
        "\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {train_loss }, Acc: {train_acc} \")\n",
        "\n",
        "        test_loss, test_acc = test_one_epoch(model, test_loader, criterion)\n",
        "        print(f\"\\tValidation Loss: {test_loss }, Acc: {test_acc} \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "j9k06bjfcypD"
      },
      "outputs": [],
      "source": [
        "# --- Convert numpy.array to torch.tensor\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create TensorDatasets for train and test data\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create DataLoaders for train and test data\n",
        "train_loader = DataLoader(train_dataset, batch_size=param.batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=param.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlrDTuk5K65Q"
      },
      "source": [
        "# First model\n",
        "\n",
        "\n",
        "\n",
        "In the first model, you will successively\n",
        "- step-1) learn word embeddings $e^{<t>}$ of each item of the inut sequence $x^{(t)}$.\n",
        "    - This is done by learning an embedding matrix $E$. You will use the `nn.Embedding` layer in pytorch.\n",
        "    - In pytorch, the `nn.Embedding` layer does not really perform a matrix multiplication going from one-hot-encoding to embedding (it would be very costly to do that).\n",
        "    - In pytorch `nn.Embedding` is a special layer that goes directly from index-of-the-word-in-the-dictionary to the embedding $e^{(t)}$\n",
        "    - The embedding goes from `param.n_word` dimensions to  `param.n_embedding` dimensions\n",
        "- step-2) compute the average over time $t$ of the embedding $e^{(t)}$ obtained for each word $x^{(t)}$ of a sequence (you should use `torch.mean`)\n",
        "- step-3) apply a fully connected (`nn.linear` layer in pytorch) which output activation is a sigmoid (predicting the 0 or 1 rating)\n",
        "\n",
        "<img src=\"https://perso.telecom-paristech.fr/gpeeters/doc/Lab_DL_RNN_01.png\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPh7ry4Hajvt"
      },
      "outputs": [],
      "source": [
        "nn.Embedding?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zspaUptgtW9l",
        "outputId": "1da7df42-6d19-42ae-c671-80a9e79726ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 100])\n",
            "torch.Size([64, 1])\n"
          ]
        }
      ],
      "source": [
        "if student:\n",
        "    # --- START CODE HERE (02)\n",
        "    class SimpleModel(nn.Module):\n",
        "        def __init__(self, param):\n",
        "            super(SimpleModel, self).__init__()\n",
        "            self.embd = nn.Embedding(param.n_word, param.n_embedding, max_norm=True)\n",
        "            self.linear = nn.Linear(100, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.embd(x)\n",
        "            x = torch.mean(x, [2])\n",
        "            x = self.linear(x)\n",
        "            out = torch.sigmoid(x)\n",
        "            return out\n",
        "\n",
        "    # --- END CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "# --- Test\n",
        "torch.manual_seed(0)\n",
        "model = SimpleModel(param)\n",
        "print(X_train_tensor[:param.batch_size, :].size())\n",
        "print(model(X_train_tensor[:param.batch_size, :]).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-RsBZDShCyag"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(),param.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqQJLiu2c8_M",
        "outputId": "3e343285-54f0-4129-9492-e7b5cb7d29a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.6899655421676538, Acc: 0.5527573529411764 \n",
            "\tValidation Loss: 0.6801857283658079, Acc: 0.6135949488491049 \n",
            "Epoch 2, Loss: 0.6191653465980764, Acc: 0.7079523657289002 \n",
            "\tValidation Loss: 0.5661355933112562, Acc: 0.747786125319693 \n",
            "Epoch 3, Loss: 0.5098490371271167, Acc: 0.7887148337595907 \n",
            "\tValidation Loss: 0.492083014658345, Acc: 0.7899616368286445 \n",
            "Epoch 4, Loss: 0.44819400209904936, Acc: 0.8196531329923273 \n",
            "\tValidation Loss: 0.45478515849089074, Acc: 0.8011748721227621 \n",
            "Epoch 5, Loss: 0.410706848134775, Acc: 0.8368686061381074 \n",
            "\tValidation Loss: 0.4311233018822682, Acc: 0.8126278772378517 \n",
            "Epoch 6, Loss: 0.38550961352980045, Acc: 0.8471307544757033 \n",
            "\tValidation Loss: 0.41563296790622994, Acc: 0.8202845268542199 \n",
            "Epoch 7, Loss: 0.3674525448199733, Acc: 0.8531569693094629 \n",
            "\tValidation Loss: 0.40635581104956625, Acc: 0.8239050511508952 \n",
            "Epoch 8, Loss: 0.3537236739454977, Acc: 0.8575447570332481 \n",
            "\tValidation Loss: 0.39919744390051076, Acc: 0.8256633631713556 \n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "train(model, train_loader, test_loader, criterion, optimizer, param.n_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBqyzLJRUIsC"
      },
      "source": [
        "## Results\n",
        "\n",
        "After only 8 epochs, you should obtain an accuracy \"around\" 86.7%/ 84.5% for the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRP-h4Xr_HNJ"
      },
      "source": [
        "## Using the trained embedding to find equivalence between words\n",
        "\n",
        "Since the embedding is part of the models, we can look at the trained embedding matrix $E$ and use it to get the most similar words (according to the trained matrix $E$) in the dictionary.\n",
        "You will use the weights of the `nn.Embedding` layer to find the most similar words to `great`. We will use an Euclidean distance for that.\n",
        "\n",
        "- 1) Retrieve the weights of the `nn.Embedding` layer\n",
        "- 2) Get the position of `great` in the dictionary\n",
        "- 3) Knowing this position, get the word-embedding of `great`\n",
        "- 4) Find (using Euclidean distance), the closest embedded-words to `great`\n",
        "\n",
        "Remarks:\n",
        "- you can access a specific layer of the model by using the name you used to define `self.??? = nn.Embedding` in the `__init__`method of `SimpleModel`.\n",
        "- be careful about the order of the dimensions of the embedding matrix `E`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xMubRqJ_HNJ",
        "outputId": "db300443-de9f-4150-d590-1b4e857c4737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1640, -0.1680, -0.0332,  ...,  0.2415,  0.1458, -0.1218],\n",
            "        [-0.0336,  0.0560, -0.0168,  ..., -0.1198,  0.3044, -0.0250],\n",
            "        [-0.1215,  0.1097,  0.1393,  ..., -0.1816, -0.0279,  0.0771],\n",
            "        ...,\n",
            "        [ 0.3289,  0.2391, -0.0988,  ..., -0.1944,  0.1962,  0.1123],\n",
            "        [-0.0966,  0.0201, -0.1079,  ..., -0.1554,  0.0996, -0.0401],\n",
            "        [-0.2571, -0.1490, -0.2194,  ..., -0.2288, -0.1202, -0.2196]])\n"
          ]
        }
      ],
      "source": [
        "if student:\n",
        "    # --- START CODE HERE (03)\n",
        "    E = model.embd.weight.data\n",
        "    pos = word_to_id[\"great\"]\n",
        "    embd_great = E[pos]\n",
        "\n",
        "    print(E)\n",
        "\n",
        "    # --- END CODE HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7_bofT5vSk5",
        "outputId": "0bc11404-4d9e-4011-f329-e604ec01dc43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5000, 32])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "E.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeOo2JYVpACW",
        "outputId": "a7618375-f7e1-4e4a-b7b2-a23ea11e06bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1768, 0.1768, 0.1768, 0.1768, 0.1768, 0.1768, 0.1768, 0.1768, 0.1768,\n",
              "        0.1768, 0.1768, 0.1768, 0.1768, 0.1768, 0.1768, 0.1768, 0.1768, 0.1768,\n",
              "        0.1768, 0.1768, 0.1768, 0.1768, 0.1768, 0.1768, 0.1768, 0.1768, 0.1768,\n",
              "        0.1768, 0.1768, 0.1768, 0.1768, 0.1768])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "E[pos, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 5 most similar words to 'great' are:\n",
            "wonderful\n",
            "excellent\n",
            "superb\n",
            "fantastic\n",
            "best\n"
          ]
        }
      ],
      "source": [
        "# let's search for the most similar words to \"great\"\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similar_words = np.argsort(cosine_similarity(embd_great.reshape(1, -1), E).squeeze())[::-1][:5]\n",
        "print(\"The 5 most similar words to 'great' are:\")\n",
        "for i in similar_words:\n",
        "    print(id_to_word[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK9e5Eo1Ks2a"
      },
      "source": [
        "# Second model\n",
        "\n",
        "In the second model, you will replace step-2 (which was \"compute the average over time $t$ of the embedding $e^{(t)}$\") by a RNN layer over time.\n",
        "More precisely you will use a LSTM (`nn.LSTM` layer in pytorch) with `param.n_lstm=100` units (or dimensions) in a Many-To-One configuration\n",
        "\n",
        "Don't forget that in ou data, the first dimension of `X_train/X_test` represents the batch (`batch_first=True` in pytorch).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dl-CSMKoViX",
        "outputId": "f00aebbe-878c-4fa4-8cf4-c5d5d2943cf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 100])\n",
            "torch.Size([64, 1])\n"
          ]
        }
      ],
      "source": [
        "param.n_lstm=100\n",
        "\n",
        "if student:\n",
        "    # --- START CODE HERE (04)\n",
        "    class LstmModel(nn.Module):\n",
        "        def __init__(self, param):\n",
        "            super(LstmModel, self).__init__()\n",
        "            self.embd = nn.Embedding(param.n_word, param.n_embedding, max_norm=True)\n",
        "            self.lstm = nn.LSTM(param.n_embedding, 100, batch_first=True)\n",
        "            self.linear = nn.Linear(100, 1)\n",
        "\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.embd(x)\n",
        "            _, (x, _) = self.lstm(x)\n",
        "            x = x[-1]\n",
        "            x = self.linear(x)\n",
        "            out = torch.sigmoid(x)\n",
        "            return out\n",
        "    # --- END CODE HERE\n",
        "\n",
        "\n",
        "# --- Test\n",
        "torch.manual_seed(0)\n",
        "model = LstmModel(param)\n",
        "print(X_train_tensor[:param.batch_size, :].size())\n",
        "print(model(X_train_tensor[:param.batch_size, :]).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bp7PzX7oXtB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), param.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP8TvdgNiUQd",
        "outputId": "24106108-c45b-4f29-ba90-f13ac966d206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.5870881499079488, Acc: 0.6691096547314578 \n",
            "\tValidation Loss: 0.5262398856222782, Acc: 0.7276054987212276 \n",
            "Epoch 2, Loss: 0.37851566007679993, Acc: 0.8358455882352942 \n",
            "\tValidation Loss: 0.37104064580577106, Acc: 0.8380434782608696 \n",
            "Epoch 3, Loss: 0.3177442296844004, Acc: 0.8666480179028133 \n",
            "\tValidation Loss: 0.3717850907836729, Acc: 0.8420556265984654 \n",
            "Epoch 4, Loss: 0.28448391149339775, Acc: 0.8832400895140665 \n",
            "\tValidation Loss: 0.3641240746926164, Acc: 0.8441895780051151 \n",
            "Epoch 5, Loss: 0.2652340763822541, Acc: 0.8922394501278773 \n",
            "\tValidation Loss: 0.39081710942870823, Acc: 0.8444533248081841 \n",
            "Epoch 6, Loss: 0.25335152738768124, Acc: 0.8984814578005115 \n",
            "\tValidation Loss: 0.3871535580328968, Acc: 0.8410326086956522 \n",
            "Epoch 7, Loss: 0.23015001084645995, Acc: 0.9086636828644502 \n",
            "\tValidation Loss: 0.42387004299541875, Acc: 0.8366128516624042 \n",
            "Epoch 8, Loss: 0.21963614678901175, Acc: 0.9149056905370844 \n",
            "\tValidation Loss: 0.424953333023564, Acc: 0.8410166240409207 \n"
          ]
        }
      ],
      "source": [
        " # Train the model\n",
        "train(model, train_loader, test_loader, criterion, optimizer, param.n_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1LN_fjMWBHJ"
      },
      "source": [
        "## Results\n",
        "\n",
        "After only 8 epochs, you should obtain an accuracy around 91.1%/ 84.7% for the test data."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
