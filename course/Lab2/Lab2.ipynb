{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaht-FPA1Jvq"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "## Lab2: Train a Convolutional Neural Network (CNN).\n",
        "\n",
        "In this Lab session we will learn how to train a CNN from scratch for classifying MNIST digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UvxtTYHlVfRK"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYCvhGxKWyN7"
      },
      "source": [
        "### Define LeNet\n",
        "\n",
        "![network architecture](https://www.researchgate.net/profile/Lucijano-Berus/publication/329891470/figure/fig1/AS:707347647307776@1545656229128/Architecture-of-LeNet-5-a-Convolutional-Neural-Network-for-digits-digits-recognition-An.ppm)\n",
        "\n",
        "Here we are going to define our first CNN which is **LeNet** in this case. This architecture has been introduced and is detailed in [this article](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf). To construct a LeNet we will be using some convolutional layers followed by some fully-connected layers. The convolutional layers can be simply defined using `torch.nn.Conv2d` module of `torch.nn` package. Details can be found [here](https://pytorch.org/docs/stable/nn.html#conv2d). Moreover, we will use pooling operation to reduce the size of convolutional feature maps. For this case we are going to use `torch.nn.functional.max_pool2d`. Details about maxpooling can be found [here](https://pytorch.org/docs/stable/nn.html#max-pool2d)\n",
        "\n",
        "Differently from our previous Lab, we will use a Rectified Linear Units (ReLU) as activation function with the help of `torch.nn.functional.relu`, replacing `torch.nn.Sigmoid`. Details about ReLU can be found [here](https://pytorch.org/docs/stable/nn.html#id26)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dMC_LDYdWkI7"
      },
      "outputs": [],
      "source": [
        "class LeNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "\n",
        "    # input channel = 1, output channels = 6, kernel size = 5\n",
        "    # input image size = (32, 32), image output size = (28, 28)\n",
        "    self.layer1 = torch.nn.Conv2d(1, 6, 5, stride=1, padding=2)\n",
        "\n",
        "    # input channel = 6, output channels = 6, kernel size = 2\n",
        "    # input image size = (28, 28), output image size = (14, 14)\n",
        "    # see maxpooling\n",
        "\n",
        "    # input dim = ( 14 x 14 x 6), output dim = (10 x 10 x 16)\n",
        "    self.layer2 = torch.nn.Conv2d(6, 16, 5, stride=1)\n",
        "\n",
        "    # input dim = (10 x 10 x 16), output dim = (5 x 5 x 16)\n",
        "    # see maxpooling\n",
        "\n",
        "    # input dim =(5x5x16) , output dim = 120\n",
        "    self.fc1 = torch.nn.Linear(5*5*16, 120)\n",
        "\n",
        "    self.fc2 = torch.nn.Linear(120, 84)\n",
        "    self.fc3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    O1 = self.layer1(x)\n",
        "    A1 = torch.nn.functional.relu(O1)\n",
        "\n",
        "    O2 = torch.nn.functional.max_pool2d(A1, 2)\n",
        "\n",
        "    O3 = self.layer2(O2)\n",
        "    A3 = torch.nn.functional.relu(O3)\n",
        "\n",
        "    x = torch.nn.functional.max_pool2d(A3, 2)\n",
        "\n",
        "    # flatten the feature maps into a long vector\n",
        "    x = x.view(x.shape[0], -1)\n",
        "\n",
        "    O5 = self.fc1(x)\n",
        "    A5 = torch.nn.functional.relu(O5)\n",
        "    O6 = self.fc2(A5)\n",
        "    A6 = torch.nn.functional.relu(O6)\n",
        "    x = self.fc3(A6)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gChf6TvWonrV"
      },
      "source": [
        "### Define cost function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6j5UrBH3oek8"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.functional.cross_entropy\n",
        "  return cost_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2TjXeVdorV9"
      },
      "source": [
        "### Define the optimizer\n",
        "\n",
        "We will use SGD with learning rate-lr, weight_decay=wd and  momentum=momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hBZN-WPboulR"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(),\n",
        "                      lr=lr,\n",
        "                      momentum=momentum,\n",
        "                      weight_decay=wd)\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTkfrV64oxIL"
      },
      "source": [
        "### Train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t-sE5vFio0lf"
      },
      "outputs": [],
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "\n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # Reset the optimizer\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6IT0Lsgo8AM"
      },
      "source": [
        "### Define the function that fetches a data loader that is then used during iterative training.\n",
        "\n",
        "We will learn a new thing in this function as how to Normalize the inputs given to the network.\n",
        "\n",
        "***Why Normalization is needed***?\n",
        "\n",
        "To have nice and stable training of the network it is recommended to normalize the network inputs between \\[-1, 1\\].\n",
        "\n",
        "***How it can be done***?\n",
        "\n",
        "This can be simply done using `torchvision.transforms.Normalize()` transform. Details can be found [here](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qDxpo6uVo_8k"
      },
      "outputs": [],
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "\n",
        "  # Prepare data transformations and then combine them sequentially\n",
        "  transform = list()\n",
        "  transform.append(T.ToTensor())                            # converts Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # Normalizes the Tensors between [-1, 1]\n",
        "  transform = T.Compose(transform)                          # Composes the above transformations into one.\n",
        "\n",
        "  # Load data\n",
        "  full_training_data = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
        "  test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)\n",
        "\n",
        "\n",
        "  # Create train and validation splits\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHcB8f0AsY4n"
      },
      "source": [
        "### Wrapping everything up\n",
        "\n",
        "Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ip_R-hruse0Q"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: Size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "'''\n",
        "\n",
        "def main(batch_size=128,\n",
        "         device='cuda:0',\n",
        "         learning_rate=0.01,\n",
        "         weight_decay=0.000001,\n",
        "         momentum=0.9,\n",
        "         epochs=50):\n",
        "\n",
        "  train_loader, val_loader, test_loader = get_data(batch_size)\n",
        "\n",
        "  # TODO for defining LeNet-5\n",
        "  net = LeNet()\n",
        "  net.to(device)\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltdCMiB3t18h"
      },
      "source": [
        "Lets train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "6d-z20H4tziL",
        "outputId": "3be1fcda-d768-4a44-b32e-9ccb4b5cc1ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 6954291.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 356909.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:02<00:00, 717611.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 1759864.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[9], line 21\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(batch_size, device, learning_rate, weight_decay, momentum, epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# TODO for defining LeNet-5\u001b[39;00m\n\u001b[1;32m     20\u001b[0m net \u001b[38;5;241m=\u001b[39m LeNet()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m get_optimizer(net, learning_rate, weight_decay, momentum)\n\u001b[1;32m     24\u001b[0m cost_function \u001b[38;5;241m=\u001b[39m get_cost_function()\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 293\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "net = main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQBDT48CKMVC"
      },
      "source": [
        "Using the proper metric from sklearn, check which character is most frequently confused with which: can you explain why ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm5b4MV1KzQ7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 128\n",
        "# Ensure the model is in evaluation mode\n",
        "net.eval()\n",
        "\n",
        "train_loader, val_loader, test_loader = get_data(batch_size)\n",
        "\n",
        "# Collect all predictions and true labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "\n",
        "  # Find the most confused classes\n",
        "  most_confused = np.unravel_index(np.argmax(conf_matrix, axis=None), conf_matrix.shape)\n",
        "  most_confused_class_1 = most_confused[0]\n",
        "  most_confused_class_2 = most_confused[1]\n",
        "  print(f\"The most confused classes are {most_confused_class_1} and {most_confused_class_2}\")\n",
        "  # Plot confusion matrix\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('True')\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOvixkfeMHrD"
      },
      "source": [
        "The LeNet5 architecture can also be implemented using the sequential API ([see documentation ](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)). Reimplement it with this API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i3-pC5xAyu5"
      },
      "source": [
        "##Experiments\n",
        "\n",
        "\n",
        "* Implement adaptive early stopping: if the validation loss did not decrease for K consecutive epochs, stop training.\n",
        "* Change dataset in order to evaluate the LeNet5 network on cifar10 dataset. You can have a look at the pytorch to easily access the cifar10 dataset.\n",
        "* Try to improve performance with:\n",
        "   *   data-augmentation\n",
        "   *   dropout\n",
        "* Implement the resnet18 architecture using the Resnet18 class from pytorch.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QptU-6pFjWcY"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: Size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "'''\n",
        "\n",
        "def main_early_stopping(batch_size=128,\n",
        "         device='cuda:0',\n",
        "         learning_rate=0.01,\n",
        "         weight_decay=0.000001,\n",
        "         momentum=0.9,\n",
        "         epochs=50,\n",
        "        K=5):\n",
        "\n",
        "  train_loader, val_loader, test_loader = get_data(batch_size)\n",
        "\n",
        "  # TODO for defining LeNet-5\n",
        "  net = LeNet()\n",
        "  net.to(device)\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  val_los = float('inf')\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "\n",
        "    if(val_loss_last < val_los):\n",
        "      val_loss_last = val_los\n",
        "      count = 0\n",
        "    else:\n",
        "      count +=1\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    if(count >= K):\n",
        "      break\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # Ensure the model is in evaluation mode\n",
        "  net.eval()\n",
        "\n",
        "  # Collect all predictions and true labels\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "          inputs, labels = data\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          outputs = net(inputs)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          all_preds.extend(predicted.cpu().numpy())\n",
        "          all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  # Calculate confusion matrix\n",
        "  conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "  # Find the most confused classes\n",
        "  most_confused = np.unravel_index(np.argmax(conf_matrix, axis=None), conf_matrix.shape)\n",
        "  most_confused_class_1 = most_confused[0]\n",
        "  most_confused_class_2 = most_confused[1]\n",
        "  print(f\"The most confused classes are {most_confused_class_1} and {most_confused_class_2}\")\n",
        "  # Plot confusion matrix\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('True')\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
